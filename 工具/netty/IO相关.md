# I/O模型 (unix 5种I/O模型)

#### 阻塞I/O模型

```
最常见的I/O模型，所有文件操作都是阻塞，套接字为例：在进程空间调用recvfrom,其系统调用知道数据包到达且被复制到应用程序的缓冲中或者发生错误才返回，在此期间一直会等待。
```

#### 非阻塞I/O模型

```
recvfrom从应用层到内核的时候，如果该缓冲区没有数据的话，就直接返回一个EWOULDBLOCK错误，一般对非阻塞I/O模型进行轮询检查这个状态，看内核是不是有数据到来。
```

#### I/O复用模型

```
linux提供select/poll，进程通过将一个或多个fd传递给select或者poll系统调用，阻塞在select操作上，这样select/poll可以帮我们侦测多个fd是否处于就绪状态。select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限，因此它使用受到了一些限制，linux提供了一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高，当有fd就绪时，立即回调函数rollback
```

#### 信号驱动I/O模型

```
首先开启套接字接口驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，它是非阻塞的）。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理数据
```

#### 异步I/O

```
告知内核启动某个操作，并让内核在整个操作完成后（包括将数据从内核复制到用户自己的缓冲区）通知我们。这种模型与信号驱动的主要区别是：信号驱动I/O由内核通知我们何时开始一个I/O操作；异步I/O模型由内核通知我们I/O操作何时完成
```



# I/O多路复用技术

```
一、在I/O编程过程中，当需要同时处理多个客户端接入请求，可以利用多线程或者I/O多路复用技术进行处理，I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求，系统不需要创建新的进程或者线程，降低了维护工作。场景
   服务器需要同时处理多个处于监听状态或者多个连接状态的套接字
   服务器需要同时处理多种网络协议的套接字
二、目前支持I/O多路复用的系统调用有selct、pselect、poll、epoll,在linux编程中很长一段时select做轮询和网络事件通知，然而select的一些固有缺陷导致它的应用受到很大的限制，最终选择了epoll，现总结如下。

   1. 支持一个进程打开socket描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数）。
   select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。对于那些需要支持上万个TCP连接的大型服务外企来说显然太少了。可以选择修改这个宏然后重新编译内核，不过这将会带来网络效率的下降。我们也可以通过选择进程的方案（传统Apache方案）解决这个问题，不过虽然linux上创建进程的代价比较小，但仍然是不可忽视的。另外进程间的数据交换非常麻烦，对于java来说，由于没有共享内存，需要通过socket通信或者其他地方进行数据同步，这带来了额外的性能损耗，增加了程序的复杂性，所以也不是一种完美的解决方案。值得庆幸的是，epoll并没有这个限制，它支持的FD上线是操作系统的最大文件句柄数，这个数字远远大于1024。例如，在1GB内存的机器上大约是10万个句柄左右，具体的值可以通过cat /proc/sys/fs/file-max 察看，通常情况下这个值跟系统的内存关系比较大。
   2. I/O效率不会随着FD数目的增加而线性下降。
   传统的select/poll的另外一个致命的弱点，就是当你拥有一个很大的socket集合时，由于网络延迟或者链路空闲，任一时刻只有少部分的socket是“活跃”的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。epoll不存在这个问题，它只会对“活跃”的socket进行操作--这是因为在内核的实现中，epoll是根据每个fd上面的callback函数实现的。那么，只有“活跃”的socket才会去主动调用callback函数，其他idle状态的socket则不会。在这点上，epoll实现了一个伪AIO。针对epoll和select性能对比的benchmark测试表明：如果所有的socket都处于活跃状态--例如一个高速LAN环境，epoll并不比select/poll效率高太多；相反，如果过多使用epoll_ctl效率相比还有稍微降低。但是一旦使用idle connections 模拟WAN环境，epoll的效率就远在select/poll之上了。
   3. 使用mmap加速内核与用户空闲的消息传递。
   无论是select、poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存复制就显得非常重要，epoll是通过内核和用户空间mmap同一块内存来实现的。
   

```



# 零拷贝

```
   零拷贝在 I/O 复用模型中，执行读写 I/O 操作依然是阻塞的，在执行读写 I/O 操作时，存在着多次内存拷贝和上下文切换，给系统增加了性能开销。
   
   零拷贝是一种避免多次内存复制的技术，用来优化读写 I/O 操作。
   
   在网络编程中，通常由 read、write 来完成一次 I/O 读写操作。每一次 I/O 读写操作都需要完成四次内存拷贝，路径是 I/O 设备 -> 内核空间 -> 用户空间 -> 内核空间 -> 其它 I/O 设备。
   
   Linux 内核中的 mmap 函数可以代替 read、write 的 I/O 读写操作，实现用户空间和内核空间共享一个缓存数据。mmap 将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址，不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理内存地址。这种方式避免了内核空间与用户空间的数据交换。I/O 复用中的 epoll 函数中就是使用了 mmap 减少了内存拷贝。
   
   在 Java 的 NIO 编程中，则是使用到了 Direct Buffer 来实现内存的零拷贝。Java 直接在 JVM 内存空间之外开辟了一个物理内存空间，这样内核和用户进程都能共享一份缓存数据。这是在 08 讲中已经详细讲解过的内容，你可以再去回顾下。
```

#### epoll原理

```
参考资料

第一部分：select和epoll的任务 
关键词：应用程序 文件句柄 用户态 内核态  监控者
要比较epoll相比较select高效在什么地方，就需要比较二者做相同事情的方法。
要完成对I/O流的复用需要完成如下几个事情：
1.用户态怎么将文件句柄传递到内核态？
2.内核态怎么判断I/O流可读可写？
3.内核怎么通知监控者有I/O流可读可写？
4.监控者如何找到可读可写的I/O流并传递给用户态应用程序？
5.继续循环时监控者怎样重复上述步骤？
搞清楚上述的步骤也就能解开epoll高效的原因了。

select的做法：

步骤1的解法：select创建3个文件描述符集，并将这些文件描述符拷贝到内核中，这里限制了文件句柄的最大的数量为1024（注意是全部传入---第一次拷贝）；

步骤2的解法：内核针对读缓冲区和写缓冲区来判断是否可读可写,这个动作和select无关；

步骤3的解法：内核在检测到文件句柄可读/可写时就产生中断通知监控者select，select被内核触发之后，就返回可读可写的文件句柄的总数；

步骤4的解法：select会将之前传递给内核的文件句柄再次从内核传到用户态（第2次拷贝），select返回给用户态的只是可读可写的文件句柄总数，再使用FD_ISSET宏函数来检测哪些文件I/O可读可写（遍历）；
步骤5的解法：select对于事件的监控是建立在内核的修改之上的，也就是说经过一次监控之后，内核会修改位，因此再次监控时需要再次从用户态向内核态进行拷贝（第N次拷贝）

epoll的做法：

步骤1的解法：首先执行epoll_create在内核专属于epoll的高速cache区，并在该缓冲区建立红黑树和就绪链表，用户态传入的文件句柄将被放到红黑树中（第一次拷贝）。

步骤2的解法：内核针对读缓冲区和写缓冲区来判断是否可读可写，这个动作与epoll无关；

步骤3的解法：epoll_ctl执行add动作时除了将文件句柄放到红黑树上之外，还向内核注册了该文件句柄的回调函数，内核在检测到某句柄可读可写时则调用该回调函数，回调函数将文件句柄放到就绪链表。

步骤4的解法：epoll_wait只监控就绪链表就可以，如果就绪链表有文件句柄，则表示该文件句柄可读可写，并返回到用户态（少量的拷贝）；

步骤5的解法：由于内核不修改文件句柄的位，因此只需要在第一次传入就可以重复监控，直到使用epoll_ctl删除，否则不需要重新传入，因此无多次拷贝。
简单说：epoll是继承了select/poll的I/O复用的思想，并在二者的基础上从监控IO流、查找I/O事件等角度来提高效率，具体地说就是内核句柄列表、红黑树、就绪list链表来实现的。

第二部分：epoll详解

先简单回顾下如何使用C库封装的3个epoll系统调用吧。

1、int epoll_create(int size);  
2、int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  
3、int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);  

使用起来很清晰：

A.epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。

B.epoll_ctl可以操作上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等(也就是将I/O流放到内核)。

C.epoll_wait在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时，就返回用户态的进程（也就是在内核层面捕获可读写的I/O事件）。

从上面的调用方式就可以看到epoll比select/poll的优越之处：
因为后者每次调用时都要传递你所要监控的所有socket给select/poll系统调用，这意味着需要将用户态的socket列表copy到内核态，如果以万计的句柄会导致每次都要copy几十几百KB的内存到内核态，非常低效。而我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。

====>select监控的句柄列表在用户态，每次调用都需要从用户态将句柄列表拷贝到内核态，但是epoll中句柄就是建立在内核中的，这样就减少了内核和用户态的拷贝，高效的原因之一。

所以，实际上在你调用epoll_create后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。

在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。

epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。

epoll高效的原因：      

这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件.     

当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已.      

那么，这个准备就绪list链表是怎么维护的呢？       

当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。

epoll综合的执行过程：       

如此，一棵红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。

epoll水平触发和边缘触发的实现：     
当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表， 最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了，所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。

====>区别就在于epoll_wait将socket返回到用户态时是否情况就绪链表。  

第三部分：epoll高效的本质

1.减少用户态和内核态之间的文件句柄拷贝；
2.减少对可读可写文件句柄的遍历；
```



#### 参考资料

```
《netty权威指南》
https://www.zhihu.com/question/20122137
https://time.geekbang.org/column/article/100861
```

