# I/O模型 (unix 5种I/O模型)

#### 阻塞I/O模型

```
最常见的I/O模型，所有文件操作都是阻塞，套接字为例：在进程空间调用recvfrom,其系统调用知道数据包到达且被复制到应用程序的缓冲中或者发生错误才返回，在此期间一直会等待。
```

#### 非阻塞I/O模型

```
recvfrom从应用层到内核的时候，如果该缓冲区没有数据的话，就直接返回一个EWOULDBLOCK错误，一般对非阻塞I/O模型进行轮询检查这个状态，看内核是不是有数据到来。
```

#### I/O复用模型

```
linux提供select/poll，进程通过将一个或多个fd传递给select或者poll系统调用，阻塞在select操作上，这样select/poll可以帮我们侦测多个fd是否处于就绪状态。select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限，因此它使用受到了一些限制，linux提供了一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高，当有fd就绪时，立即回调函数rollback
```

#### 信号驱动I/O模型

```
首先开启套接字接口驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，它是非阻塞的）。当数据准备就绪时，就为该进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理数据
```

#### 异步I/O

```
告知内核启动某个操作，并让内核在整个操作完成后（包括将数据从内核复制到用户自己的缓冲区）通知我们。这种模型与信号驱动的主要区别是：信号驱动I/O由内核通知我们何时开始一个I/O操作；异步I/O模型由内核通知我们I/O操作何时完成
```



# I/O多路复用技术

```
一、在I/O编程过程中，当需要同时处理多个客户端接入请求，可以利用多线程或者I/O多路复用技术进行处理，I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求，系统不需要创建新的进程或者线程，降低了维护工作。场景
   服务器需要同时处理多个处于监听状态或者多个连接状态的套接字
   服务器需要同时处理多种网络协议的套接字
二、目前支持I/O多路复用的系统调用有selct、pselect、poll、epoll,在linux编程中很长一段时select做轮询和网络事件通知，然而select的一些固有缺陷导致它的应用受到很大的限制，最终选择了epoll，现总结如下。

   1. 支持一个进程打开socket描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数）。
   select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024。对于那些需要支持上万个TCP连接的大型服务外企来说显然太少了。可以选择修改这个宏然后重新编译内核，不过这将会带来网络效率的下降。我们也可以通过选择进程的方案（传统Apache方案）解决这个问题，不过虽然linux上创建进程的代价比较小，但仍然是不可忽视的。另外进程间的数据交换非常麻烦，对于java来说，由于没有共享内存，需要通过socket通信或者其他地方进行数据同步，这带来了额外的性能损耗，增加了程序的复杂性，所以也不是一种完美的解决方案。值得庆幸的是，epoll并没有这个限制，它支持的FD上线是操作系统的最大文件句柄数，这个数字远远大于1024。例如，在1GB内存的机器上大约是10万个句柄左右，具体的值可以通过cat /proc/sys/fs/file-max 察看，通常情况下这个值跟系统的内存关系比较大。
   2. I/O效率不会随着FD数目的增加而线性下降。
   传统的select/poll的另外一个致命的弱点，就是当你拥有一个很大的socket集合时，由于网络延迟或者链路空闲，任一时刻只有少部分的socket是“活跃”的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。epoll不存在这个问题，它只会对“活跃”的socket进行操作--这是因为在内核的实现中，epoll是根据每个fd上面的callback函数实现的。那么，只有“活跃”的socket才会去主动调用callback函数，其他idle状态的socket则不会。在这点上，epoll实现了一个伪AIO。针对epoll和select性能对比的benchmark测试表明：如果所有的socket都处于活跃状态--例如一个高速LAN环境，epoll并不比select/poll效率高太多；相反，如果过多使用epoll_ctl效率相比还有稍微降低。但是一旦使用idle connections 模拟WAN环境，epoll的效率就远在select/poll之上了。
   3. 使用mmap加速内核与用户空闲的消息传递。
   无论是select、poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存复制就显得非常重要，epoll是通过内核和用户空间mmap同一块内存来实现的。
   

```



# 零拷贝

```
   零拷贝在 I/O 复用模型中，执行读写 I/O 操作依然是阻塞的，在执行读写 I/O 操作时，存在着多次内存拷贝和上下文切换，给系统增加了性能开销。
   
   零拷贝是一种避免多次内存复制的技术，用来优化读写 I/O 操作。
   
   在网络编程中，通常由 read、write 来完成一次 I/O 读写操作。每一次 I/O 读写操作都需要完成四次内存拷贝，路径是 I/O 设备 -> 内核空间 -> 用户空间 -> 内核空间 -> 其它 I/O 设备。
   
   Linux 内核中的 mmap 函数可以代替 read、write 的 I/O 读写操作，实现用户空间和内核空间共享一个缓存数据。mmap 将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址，不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理内存地址。这种方式避免了内核空间与用户空间的数据交换。I/O 复用中的 epoll 函数中就是使用了 mmap 减少了内存拷贝。
   
   在 Java 的 NIO 编程中，则是使用到了 Direct Buffer 来实现内存的零拷贝。Java 直接在 JVM 内存空间之外开辟了一个物理内存空间，这样内核和用户进程都能共享一份缓存数据。这是在 08 讲中已经详细讲解过的内容，你可以再去回顾下。
```

#### epoll原理

```
https://www.zhihu.com/question/20122137
https://zhuanlan.zhihu.com/p/87843750
```



#### 参考资料

```
《netty权威指南》
https://time.geekbang.org/column/article/100861
```

